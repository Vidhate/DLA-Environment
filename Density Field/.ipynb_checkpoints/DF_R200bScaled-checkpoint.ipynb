{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing r200b scaling\n",
    "The following cells build utilities and computations to develop on the idea presented in the latest update of 25th May in the introduction.<br>\n",
    "r200b will be found from M200b and the background density.\n",
    "\n",
    "## linDensityLOS(i,d)\n",
    "The statistics developed above for LOS analysis have a redundancy due to direction. The density field on either side of a halo of interest should not be treated separately if we believe in the isotropy of the Universe. Hence, a still better estimate should be to analyse the density of the distribution around the halo of interest. This will become clear through the description of the following procedure-\n",
    "<ol>\n",
    "    <li>Lines of Sight are shot parallel to the grid through the centers of all the Halos of interest from a given mock catalogue and the overdensity amplitude is computed at every scale around the halo.</li>\n",
    "    <li>A bracket of <i>L MPc</i> is introduced symmetrically around the Halo of interest. The average linear density of total amplitude enclosed in the bracket is found and plotted against scale. In the formula below <i>i</i> iterates over all grid points contained in the enclosing bracket imposed on the Halo environment.\n",
    "        $$\\lambda(L)=\\dfrac{\\sum_{i}A_i}{L}$$\n",
    "\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell: Libraries imported and general data to be used is loaded\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "colors=['g','b','r']\n",
    "sourcePath=\"../SourceData/\"  # Will not become a member of the git commits because CIC file is 1.1GB\n",
    "fileNames=['b1','b1alpha','b1T10']\n",
    "# Loading CIC data of density field. (512^3)\n",
    "GridSize=512  # cubed of course.\n",
    "cicSource=\"cic_snap049_grid512.dat\"\n",
    "data1=np.fromfile(sourcePath+cicSource)\n",
    "data=np.reshape(data1,(GridSize,GridSize,GridSize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell: Cosmology is defined. Simulation details are listed. Other constants to be used throughout are specified.\n",
    "# Aseem's Cosmology\n",
    "sigma_8=0.811\n",
    "ns=0.961\n",
    "h=0.7\n",
    "Ob=0.045\n",
    "Om=0.276\n",
    "\n",
    "z=2.3         # Redshift specification\n",
    "M_sun=1.989e+30  # M_sun in kg\n",
    "Mega_parsec=3.086e+22  # parsec in metres\n",
    "Delta=200.0   # Overdensity definition = Delta X background\n",
    "rho_cr=((3*(100*h)**2)/(8*np.pi*6.673e-11))*((Mega_parsec/h**3)*1e+6/(M_sun/h))\n",
    "# critical density of the Universe today 3H^2/8Pi*G in units M_sun.h^-1/(MPc.h^-1)^3\n",
    "rho_m=Om*rho_cr # Units same as rho_cr. Don't need redshift considerations in comoving units i.e. Msun/h and MPc/h\n",
    "del_crit=1.69\n",
    "Lbox=150.0   # MPc/h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading master catalogue data and putting it in usbale format\n",
    "masterCat=np.loadtxt(sourcePath+\"out_49.trees\")\n",
    "\n",
    "treesID=np.array(masterCat[:,1],dtype=int)  # ID of halos in the master catalogue\n",
    "treesR200b=np.cbrt((masterCat[:,36]*3)/(4*np.pi*rho_m*200))   # The r200b of each halo from the master catalogue.\n",
    "treesMap=np.column_stack((treesID,treesR200b))\n",
    "treesMap=treesMap[treesMap[:,0].argsort()]   # TreesMap is a 2 column array with R200b for given ID of the halo\n",
    "# TreesMap is sorted wrt the ID column so that Binary Search can be done on it too save shitloads of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the given Halo catalogues from original non-pickled files to get ID's and positions\n",
    "b1=np.loadtxt(sourcePath+'b1.txt')\n",
    "b1alpha=np.loadtxt(sourcePath+'b1alpha.txt')\n",
    "b1T10=np.loadtxt(sourcePath+'b1T10.txt')\n",
    "\n",
    "b1ID,b1alphaID,b1T10ID=np.array(b1[:,0],dtype=int),np.array(b1alpha[:,0],dtype=int),np.array(b1T10[:,0],dtype=int)\n",
    "b1Pos,b1alphaPos,b1T10Pos=[],[],[]\n",
    "\n",
    "b1Pos.append(b1[:,1])\n",
    "b1Pos.append(b1[:,2])\n",
    "b1Pos.append(b1[:,3])\n",
    "b1Pos=np.array(b1Pos)\n",
    "b1Pos=b1Pos.T  # Holds the positions of all the b1 halos in order according to b1T10\n",
    "\n",
    "b1alphaPos.append(b1alpha[:,1])\n",
    "b1alphaPos.append(b1alpha[:,2])\n",
    "b1alphaPos.append(b1alpha[:,3])\n",
    "b1alphaPos=np.array(b1alphaPos)\n",
    "b1alphaPos=b1alphaPos.T\n",
    "\n",
    "b1T10Pos.append(b1T10[:,1])\n",
    "b1T10Pos.append(b1T10[:,2])\n",
    "b1T10Pos.append(b1T10[:,3])\n",
    "b1T10Pos=np.array(b1T10Pos)\n",
    "b1T10Pos=b1T10Pos.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns an array of r200b for each ID passed as an argument, in order\n",
    "# Note that treesMap is sorted on IDs and hence, we can run a Binary Search on it\n",
    "\n",
    "def findR200b(ID):\n",
    "    low,high=0,treesMap[:,0].size-1\n",
    "    while(low<=high):\n",
    "        mid=int(low+(high-low)/2)\n",
    "        if(ID==treesMap[mid,0]):\n",
    "            return(treesMap[mid,1])\n",
    "        elif(ID>treesMap[mid,0]):\n",
    "            low=mid+1\n",
    "        else:\n",
    "            high=mid-1\n",
    "    return(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helping functions for asymmetric STD calculations in averagedLOS()\n",
    "# Asymmetry in statistics. Data is split into values above and below mean. A different standard deviation is found for each set.\n",
    "\n",
    "# This function finds the STD given the mean value\n",
    "def std_dev(a,mean):\n",
    "    return np.sqrt(np.sum((a-mean)**2)/a.size)\n",
    "\n",
    "# This function returns the positive and negative STD given the total dataset and axis about which to compute\n",
    "def asymmetricSTD(a,ax):\n",
    "    means=np.mean(a,axis=ax)\n",
    "    SDPos,SDNeg=[],[]\n",
    "    for i in range(0,len(means)):\n",
    "        pos,neg=[],[]\n",
    "        currMean=means[i]\n",
    "        for j in range(0,a.shape[ax]):\n",
    "            tmp=a[j,i]   # axis specific. Assumed that j in indexed first. Need to update for generalization\n",
    "            if tmp>=currMean:\n",
    "                pos.append(tmp)\n",
    "            else:\n",
    "                neg.append(tmp)\n",
    "        SDPos.append(std_dev(np.array(pos),currMean))\n",
    "        SDNeg.append(std_dev(np.array(neg),currMean))\n",
    "\n",
    "    return np.array(SDPos),np.array(SDNeg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
