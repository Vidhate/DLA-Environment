{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing r200b scaling\n",
    "The following cells build utilities and computations to develop on the idea presented in the latest update of 25th May in the introduction.<br>\n",
    "r200b will be found from M200b and the background density.\n",
    "\n",
    "## linDensityLOS(i,d)\n",
    "The statistics developed above for LOS analysis have a redundancy due to direction. The density field on either side of a halo of interest should not be treated separately if we believe in the isotropy of the Universe. Hence, a still better estimate should be to analyse the density of the distribution around the halo of interest. This will become clear through the description of the following procedure-\n",
    "<ol>\n",
    "    <li>Lines of Sight are shot parallel to the grid through the centers of all the Halos of interest from a given mock catalogue and the overdensity amplitude is computed at every scale around the halo.</li>\n",
    "    <li>A bracket of <i>L MPc</i> is introduced symmetrically around the Halo of interest. The average linear density of total amplitude enclosed in the bracket is found and plotted against scale. In the formula below <i>i</i> iterates over all grid points contained in the enclosing bracket imposed on the Halo environment.\n",
    "        $$\\lambda(L)=\\dfrac{\\sum_{i}A_i}{L}$$\n",
    "\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell: Libraries imported and general data to be used is loaded\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "colors=['g','b','r']\n",
    "sourcePath=\"../SourceData/\"  # Will not become a member of the git commits because CIC file is 1.1GB\n",
    "fileNames=['b1','b1alpha','b1T10']\n",
    "# Loading CIC data of density field. (512^3)\n",
    "GridSize=512  # cubed of course.\n",
    "cicSource=\"cic_snap049_grid512.dat\"\n",
    "data1=np.fromfile(sourcePath+cicSource)\n",
    "data=np.reshape(data1,(GridSize,GridSize,GridSize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell: Cosmology is defined. Simulation details are listed. Other constants to be used throughout are specified.\n",
    "# Aseem's Cosmology\n",
    "sigma_8=0.811\n",
    "ns=0.961\n",
    "h=0.7\n",
    "Ob=0.045\n",
    "Om=0.276\n",
    "\n",
    "z=2.3         # Redshift specification\n",
    "M_sun=1.989e+30  # M_sun in kg\n",
    "Mega_parsec=3.086e+22  # parsec in metres\n",
    "Delta=200.0   # Overdensity definition = Delta X background\n",
    "rho_cr=((3*(100*h)**2)/(8*np.pi*6.673e-11))*((Mega_parsec/h**3)*1e+6/(M_sun/h))\n",
    "# critical density of the Universe today 3H^2/8Pi*G in units M_sun.h^-1/(MPc.h^-1)^3\n",
    "rho_m=Om*rho_cr # Units same as rho_cr. Don't need redshift considerations in comoving units i.e. Msun/h and MPc/h\n",
    "del_crit=1.69\n",
    "Lbox=150.0   # MPc/h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading master catalogue data and putting it in usbale format\n",
    "masterCat=np.loadtxt(sourcePath+\"out_49.trees\")\n",
    "\n",
    "treesID=np.array(masterCat[:,1],dtype=int)  # ID of halos in the master catalogue\n",
    "treesR200b=np.cbrt((masterCat[:,36]*3)/(4*np.pi*rho_m*200))   # The r200b of each halo from the master catalogue.\n",
    "treesMap=np.column_stack((treesID,treesR200b))\n",
    "treesMap=treesMap[treesMap[:,0].argsort()]   # TreesMap is a 2 column array with R200b for given ID of the halo\n",
    "# TreesMap is sorted wrt the ID column so that Binary Search can be done on it too save shitloads of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the given Halo catalogues from original non-pickled files to get ID's and positions\n",
    "b1=np.loadtxt(sourcePath+'b1.txt')\n",
    "b1alpha=np.loadtxt(sourcePath+'b1alpha.txt')\n",
    "b1T10=np.loadtxt(sourcePath+'b1T10.txt')\n",
    "\n",
    "b1ID,b1alphaID,b1T10ID=np.array(b1[:,0],dtype=int),np.array(b1alpha[:,0],dtype=int),np.array(b1T10[:,0],dtype=int)\n",
    "b1Pos,b1alphaPos,b1T10Pos=[],[],[]\n",
    "\n",
    "b1Pos.append(b1[:,1])\n",
    "b1Pos.append(b1[:,2])\n",
    "b1Pos.append(b1[:,3])\n",
    "b1Pos=np.array(b1Pos)\n",
    "b1Pos=b1Pos.T  # Holds the positions of all the b1 halos in order according to b1T10\n",
    "\n",
    "b1alphaPos.append(b1alpha[:,1])\n",
    "b1alphaPos.append(b1alpha[:,2])\n",
    "b1alphaPos.append(b1alpha[:,3])\n",
    "b1alphaPos=np.array(b1alphaPos)\n",
    "b1alphaPos=b1alphaPos.T\n",
    "\n",
    "b1T10Pos.append(b1T10[:,1])\n",
    "b1T10Pos.append(b1T10[:,2])\n",
    "b1T10Pos.append(b1T10[:,3])\n",
    "b1T10Pos=np.array(b1T10Pos)\n",
    "b1T10Pos=b1T10Pos.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns an array of r200b for each ID passed as an argument, in order\n",
    "# Note that treesMap is sorted on IDs and hence, we can run a Binary Search on it\n",
    "\n",
    "def findR200b(ID):\n",
    "    low,high=0,treesMap[:,0].size-1\n",
    "    while(low<=high):\n",
    "        mid=int(low+(high-low)/2)\n",
    "        if(ID==treesMap[mid,0]):\n",
    "            return(treesMap[mid,1])\n",
    "        elif(ID>treesMap[mid,0]):\n",
    "            low=mid+1\n",
    "        else:\n",
    "            high=mid-1\n",
    "    return(-1)\n",
    "print(np.array([(np.mean(10*b1r)),np.mean(10*alphar),np.mean(10*t10r)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helping functions for asymmetric STD calculations in averagedLOS()\n",
    "# Asymmetry in statistics. Data is split into values above and below mean. A different standard deviation is found for each set.\n",
    "\n",
    "# This function finds the STD given the mean value\n",
    "def std_dev(a,mean):\n",
    "    return np.sqrt(np.sum((a-mean)**2)/a.size)\n",
    "\n",
    "# This function returns the positive and negative STD given the total dataset and axis about which to compute\n",
    "def asymmetricSTD(a,ax):\n",
    "    means=np.mean(a,axis=ax)\n",
    "    SDPos,SDNeg=[],[]\n",
    "    for i in range(0,len(means)):\n",
    "        pos,neg=[],[]\n",
    "        currMean=means[i]\n",
    "        for j in range(0,a.shape[ax]):\n",
    "            tmp=a[j,i]   # axis specific. Assumed that j in indexed first. Need to update for generalization\n",
    "            if tmp>=currMean:\n",
    "                pos.append(tmp)\n",
    "            else:\n",
    "                neg.append(tmp)\n",
    "        SDPos.append(std_dev(np.array(pos),currMean))\n",
    "        SDNeg.append(std_dev(np.array(neg),currMean))\n",
    "\n",
    "    return np.array(SDPos),np.array(SDNeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The purpose of existence of this function is explained in function: averagedLOS_R200b() 2 cells later\n",
    "# This function return column wise statistics of 2 types\n",
    "# 1] mode='std' enables asymmetric standard deviation calculations about the mean value.\n",
    "# returned values are-- the mean, the std of points above mean, std of points below mean.\n",
    "# 2] mode='percentile' returns 3 values-- median, 16th percentile, 84th percentile\n",
    "\n",
    "def customStats(pos,DF,mode):\n",
    "    allModes=np.array(['std','percentile'])  # list of all available modes. Add to this list if developed further\n",
    "    if not any(mode==allModes):\n",
    "        print(\"Please enter a valid mode. Current available modes are - \"+str(allModes))\n",
    "        print(\"Jupyter Notebook kernel will be killed\")\n",
    "        exit()\n",
    "    \n",
    "    # first goal is to standardise the DF so that all 0 pos columns align together\n",
    "    maxlen=pos.size\n",
    "    standard=[]\n",
    "    for df in DF:\n",
    "        excess=maxlen-df.size\n",
    "        fill_in=np.ones(int(excess/2))*-1\n",
    "        df=np.concatenate((fill_in,df,fill_in))\n",
    "        if(df.size!=maxlen):\n",
    "            print(\"Fatal error with list sizes in DF master array.\")\n",
    "            break\n",
    "        else:\n",
    "            standard.append(df)\n",
    "    standard=np.array(standard)\n",
    "    \n",
    "    # next goal is to find the statistics according to the option given\n",
    "    standard=standard.T  # Now every list corresponds to all values of that scale\n",
    "    if mode=='std':\n",
    "        result,std_up,std_down=[],[],[]\n",
    "        for df in standard:\n",
    "            work_set=df[df!=-1]\n",
    "            res=np.mean(work_set)\n",
    "            result.append(res)\n",
    "            std_up.append(std_dev(work_set[workset>=res]))\n",
    "            std_down.append(std_dev(work_set[workset<res]))\n",
    "        return result,std_up,std_down\n",
    "            \n",
    "    elif mode=='percentile':\n",
    "        result,pnt_16,pnt_84=[],[],[]\n",
    "        for df in standard:\n",
    "            work_set=df[df!=-1]\n",
    "            res=np.median(work_set)\n",
    "            result.append(res)\n",
    "            pnt_16.append(np.percentile(work_set,16))\n",
    "            pnt_84.append(np.percentile(work_set,84))\n",
    "        return result,pnt_16,pnt_84"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell tests the code above\n",
    "p=np.array([1.,2.,3.,4.,5.,6.,7.])\n",
    "d=np.array([np.array([1.,2.,3.]),np.array([1.,2.,3.,4.,5.]),np.array([1.]),np.array([1.,2.,3.,4.,5.,6.,7.])],)\n",
    "res=customStats(p,d,'x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The LOS Method implemented has lines always passing parallel to x-axis and through the grid points\n",
    "# Direction specifies which direction the LOS are parallel to\n",
    "# r_lim specifies the limit to which LOS calculations should be done about the central position of the halo\n",
    "# Limit is specified as distance in the real position coordinates\n",
    "# Also presenting the linear density around the halo of interest\n",
    "\n",
    "def simpleR200bLOS(halo,r200b,direction='x'):\n",
    "    pos,df=[],[]\n",
    "    rlim=10*r200b  # the upper limit to which Density Field should be found\n",
    "    \n",
    "    grid2box=Lbox/GridSize   # MPc/CICcell\n",
    "    box2grid=(GridSize-1)/Lbox   # CICcells/MPc\n",
    "    \n",
    "    # Connvert Halo co-odinates from real space to closest CIC Grid positions\n",
    "    X=int(np.floor(halo[0]*box2grid))\n",
    "    Y=int(np.floor(halo[1]*box2grid))\n",
    "    Z=int(np.floor(halo[2]*box2grid))\n",
    "    \n",
    "    scale_grid=int(np.floor(-1*rlim*box2grid))   # scale_grid holds the scale at which to compute DF in CIC grid co-ods\n",
    "    scale=scale_grid*grid2box  # scale stores the conversion of scale_grid in simulation box co-ods\n",
    "    \n",
    "    while scale<=rlim:   # runs till real scale is less than  rlim. NOt we started from -rlim.\n",
    "        pos.append(scale/r200b)\n",
    "        \n",
    "        p=[]\n",
    "        if direction=='x':\n",
    "            p=np.array([X+scale_grid,Y,Z])   \n",
    "        elif direction=='y':\n",
    "            p=np.array([X,Y+scale_grid,Z])  \n",
    "        elif direction=='z':\n",
    "            p=np.array([X,Y,Z+scale_grid])   \n",
    "        else:\n",
    "            print(\"Please choose a valid direction for the LOS. Valid choices are - x | y | z\")\n",
    "            exit()\n",
    "            \n",
    "        p[p>=GridSize]=p[p>=GridSize]-GridSize\n",
    "        p[p<0]=p[p<0]+GridSize   # periodicty of the box considered\n",
    "        df.append(data[p[0],p[1],p[2]])\n",
    "        \n",
    "        scale_grid+=1\n",
    "        scale=scale_grid*grid2box\n",
    "        \n",
    "    return np.array(pos),np.array(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOS method implemented has lines passing through every halo of each catalogue to give a statistically robust estimate\n",
    "# Arguments to be passed are halo IDs and corresponding positions...\n",
    "# ...how many directions to consider for calculating the LOS - all | x | y | z\n",
    "\n",
    "def averagedLOS_R200b(ID,pos,direction='all'):\n",
    "    \n",
    "    options=np.array(['all','x','y','z'])\n",
    "    if not any(options == direction):\n",
    "        print(\"Please enter a valid value to direction argument. Valid values -- all | x | y | z\")\n",
    "        print(\"Jupyter Notebook kernel will be killed\")\n",
    "        exit()\n",
    "    \n",
    "    mocks=pos\n",
    "    masterPos,masterDF=[],[]\n",
    "    # masterDF will store LOS DF for each halo in mocks\n",
    "    # resultDF will be the mean of masterDF at each scale\n",
    "    \n",
    "    if direction == 'all':\n",
    "        for h in range(0,mocks[:,0].size):\n",
    "            for axis in ['x','y','z']:\n",
    "                scales,df=simpleR200bLOS(mocks[h],findR200b(ID[h]),direction=axis)\n",
    "                masterDF.append(df)\n",
    "                if len(scales)>len(masterPos):\n",
    "                    masterPos=scales\n",
    "        masterDF=np.array(masterDF)\n",
    "    else:\n",
    "        for h in range(0,mocks[:,0].size):\n",
    "            scales,df=simpleR200bLOS(mocks[h],findR200b(ID[h]),direction=axis)\n",
    "                masterDF.append(df)\n",
    "                if len(scales)>len(masterPos):\n",
    "                    masterPos=scales\n",
    "        masterDF=np.array(masterDF)\n",
    "    \n",
    "    # ---------------------------------------------------------------------------------------------------------------\n",
    "    # Due to the R200b scaling in this analysis, the list of DF values returned for each Halo from the catalogue...\n",
    "    #... will be different, because 2 halos can have different R200b, and hence, will span a different number of...\n",
    "    #...  CIC grid cells.\n",
    "    # Due to this I face a statistcal issue- I may not have the same number of DF values for any 2 scales.\n",
    "    # I don't wish to set a cutoff on the number of datapoints to collect and throw away important information.\n",
    "    # To get the best out of this, I define the following staistics:\n",
    "    # I stack all the centers of the halos on one column and fill DF values in both directions till I hit 10*R200b...\n",
    "    #... for that particular Halo.\n",
    "    # Now I run standard deviation etc. for each scale differently, with whatever maximum number of data points...\n",
    "    #... available to me.\n",
    "    # For this I have defined the funtion customStats() with 2 modes -- 'std' and 'percentile'\n",
    "    # ---------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    resultDF,std_up,std_below=customStats(masterPos,masterDF,mode='std') # mode='std' | mode='percentile'\n",
    "    #resultDF,prcntile16,prcntile84=runCustomStatistics(masterPos,masterDF,mode='percentile')\n",
    "    \n",
    "    return masterPos,resultDF,std_up,std_below\n",
    "    #return masterPos,resultDF,prcntile16,prcntile84"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=time.time()\n",
    "p_b1_r,df_b1_r,pt16_b1_r,pt84_b1_r=averagedLOS_R200b(b1ID,b1Pos,'x')\n",
    "p_alpha_r,df_alpha_r,pt16_alpha_r,pt84_alpha_r=averagedLOS_R200b(b1alphaID,b1alphaPos,'x')\n",
    "p_t10_r,df_t10_r,pt16_t10_r,pt84_t10_r=averagedLOS_R200b(b1T10ID,b1T10Pos,'x')\n",
    "end=time.time()\n",
    "print(\"Computed averagedLOS_R200b in \"+str(end-start)+\" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is specifically made to run the averagedLOSv2() function because of more complex plotting details.\n",
    "\n",
    "fig,ax=plt.subplots(nrows=3, ncols=1, sharex=True, sharey=True, figsize=(13,18),dpi=200)\n",
    "fig.text(0.5, 0.0, 'Distance from Halo Center (R/R200b)', ha='center',fontsize=20)\n",
    "fig.text(0.0, 0.5, 'Density Contrast', va='center', rotation='vertical',fontsize=20)\n",
    "\n",
    "plt.subplot(3,1,1)\n",
    "plt.plot(p_b1_r, df_b1_r, 'k', color='#A30000',label=\"b1\")\n",
    "plt.fill_between(p_b1_r, pt84_b1_r, pt16_b1_r,alpha=0.5, edgecolor='#FF6B6B', facecolor='#FFADAD')\n",
    "plt.ylim([0.1,3e1])\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(3,1,2)\n",
    "plt.plot(p_alpha_r, df_alpha_r, 'k', color='#00086B',label=\"Alpha\")\n",
    "plt.fill_between(p_alpha_r, pt84_alpha_r, pt16_alpha_r,alpha=0.5, edgecolor='#3a9cff', facecolor='#9bcdff')\n",
    "plt.ylim([0.1,3e1])\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(3,1,3)\n",
    "plt.plot(p_t10_r, df_t10_r, 'k', color='#036800',label=\"T10\")\n",
    "plt.fill_between(p_t10_r, pt84_t10_r, pt16_t10_r,alpha=0.5, edgecolor='#49c145', facecolor='#96ff93')\n",
    "plt.ylim([0.1,3e1])\n",
    "plt.yscale('log')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
